{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ef29f2",
   "metadata": {},
   "source": [
    "# Frozen Lake 8X8 from M. Yildiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2269665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym[toy_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e614ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Version\n",
    "\n",
    "def run(episodes, is_training=True, render=False):\n",
    "\n",
    "    env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True, render_mode='human' if render else None)\n",
    "\n",
    "    if(is_training):\n",
    "        q = np.zeros((env.observation_space.n, env.action_space.n)) # init a 64 x 4 array\n",
    "    else:\n",
    "        f = open('frozen_lake8x8.pkl', 'rb')\n",
    "        q = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    learning_rate_a = 0.9 # alpha or learning rate\n",
    "    discount_factor_g = 0.9 # gamma or discount rate. Near 0: more weight/reward placed on immediate state. Near 1: more on future state.\n",
    "    epsilon = 1         # 1 = 100% random actions\n",
    "    epsilon_decay_rate = 0.0001        # epsilon decay rate. 1/0.0001 = 10,000\n",
    "    rng = np.random.default_rng()   # random number generator\n",
    "\n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state = env.reset()[0]  # states: 0 to 63, 0=top left corner,63=bottom right corner\n",
    "        terminated = False      # True when fall in hole or reached goal\n",
    "        truncated = False       # True when actions > 200\n",
    "\n",
    "        while(not terminated and not truncated):\n",
    "            if is_training and rng.random() < epsilon:\n",
    "                action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up\n",
    "            else:\n",
    "                action = np.argmax(q[state,:])\n",
    "\n",
    "            new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "            if is_training:\n",
    "                q[state,action] = q[state,action] + learning_rate_a * (\n",
    "                    reward + discount_factor_g * np.max(q[new_state,:]) - q[state,action]\n",
    "                )\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "\n",
    "        if(epsilon==0):\n",
    "            learning_rate_a = 0.0001\n",
    "\n",
    "        if reward == 1:\n",
    "            rewards_per_episode[i] = 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    sum_rewards = np.zeros(episodes)\n",
    "    for t in range(episodes):\n",
    "        sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])\n",
    "    plt.plot(sum_rewards)\n",
    "    plt.savefig('frozen_lake8x8.png')\n",
    "\n",
    "    if is_training:\n",
    "        f = open(\"frozen_lake8x8.pkl\",\"wb\")\n",
    "        pickle.dump(q, f)\n",
    "        f.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # run(15000)\n",
    "\n",
    "    run(20000, is_training=True, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f1502",
   "metadata": {},
   "source": [
    "* +1 reward for moving closer to the goal.\n",
    "* +100 reward for reaching the goal.\n",
    "* -10 penalty for falling into a hole.\n",
    "* -1 penalty for moving farther away from the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2417b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismet\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Reward: 1 | Total Reward for Episode 1: 1\n",
      "Step Reward: 1 | Total Reward for Episode 1: 2\n",
      "Step Reward: 1 | Total Reward for Episode 1: 3\n",
      "Step Reward: 1 | Total Reward for Episode 1: 4\n",
      "Step Reward: -10 | Total Reward for Episode 1: -6\n",
      "Step Reward: 0.0 | Total Reward for Episode 2: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 2: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 2: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 2: 0.0\n",
      "Step Reward: 1 | Total Reward for Episode 2: 1.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 2: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 2: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 2: 3.0\n",
      "Step Reward: -1 | Total Reward for Episode 2: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 2: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 2: 4.0\n",
      "Step Reward: -1 | Total Reward for Episode 2: 3.0\n",
      "Step Reward: -1 | Total Reward for Episode 2: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 2: 3.0\n",
      "Step Reward: -1 | Total Reward for Episode 2: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 2: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 2: 4.0\n",
      "Step Reward: 1 | Total Reward for Episode 2: 5.0\n",
      "Step Reward: -10 | Total Reward for Episode 2: -5.0\n",
      "Step Reward: 1 | Total Reward for Episode 3: 1\n",
      "Step Reward: 1 | Total Reward for Episode 3: 2\n",
      "Step Reward: -1 | Total Reward for Episode 3: 1\n",
      "Step Reward: 1 | Total Reward for Episode 3: 2\n",
      "Step Reward: 1 | Total Reward for Episode 3: 3\n",
      "Step Reward: -1 | Total Reward for Episode 3: 2\n",
      "Step Reward: 1 | Total Reward for Episode 3: 3\n",
      "Step Reward: -1 | Total Reward for Episode 3: 2\n",
      "Step Reward: -1 | Total Reward for Episode 3: 1\n",
      "Step Reward: 1 | Total Reward for Episode 3: 2\n",
      "Step Reward: 1 | Total Reward for Episode 3: 3\n",
      "Step Reward: -1 | Total Reward for Episode 3: 2\n",
      "Step Reward: -1 | Total Reward for Episode 3: 1\n",
      "Step Reward: 0.0 | Total Reward for Episode 3: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 3: 2.0\n",
      "Step Reward: -1 | Total Reward for Episode 3: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 3: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 3: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 3: 4.0\n",
      "Step Reward: -1 | Total Reward for Episode 3: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 3: 4.0\n",
      "Step Reward: -10 | Total Reward for Episode 3: -6.0\n",
      "Step Reward: 1 | Total Reward for Episode 4: 1\n",
      "Step Reward: 0.0 | Total Reward for Episode 4: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 4: 2.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 4: 2.0\n",
      "Step Reward: -1 | Total Reward for Episode 4: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 4: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 4: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 4: 4.0\n",
      "Step Reward: 1 | Total Reward for Episode 4: 5.0\n",
      "Step Reward: -1 | Total Reward for Episode 4: 4.0\n",
      "Step Reward: 1 | Total Reward for Episode 4: 5.0\n",
      "Step Reward: 1 | Total Reward for Episode 4: 6.0\n",
      "Step Reward: -1 | Total Reward for Episode 4: 5.0\n",
      "Step Reward: -1 | Total Reward for Episode 4: 4.0\n",
      "Step Reward: -10 | Total Reward for Episode 4: -6.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 5: 0.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 4.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 3.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 3.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 3.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 4.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 3.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 3.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 3.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 1.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 0.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 1.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 5: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 1.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 5: 1.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 0.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 1.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 0.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 3.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 5: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 4.0\n",
      "Step Reward: -1 | Total Reward for Episode 5: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 4.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 5.0\n",
      "Step Reward: 1 | Total Reward for Episode 5: 6.0\n",
      "Step Reward: -10 | Total Reward for Episode 5: -4.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 6: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 6: 0.0\n",
      "Step Reward: 1 | Total Reward for Episode 6: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 6: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 6: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 6: 4.0\n",
      "Step Reward: 1 | Total Reward for Episode 6: 5.0\n",
      "Step Reward: 1 | Total Reward for Episode 6: 6.0\n",
      "Step Reward: -10 | Total Reward for Episode 6: -4.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 7: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 7: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 7: 0.0\n",
      "Step Reward: 1 | Total Reward for Episode 7: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 7: 2.0\n",
      "Step Reward: -1 | Total Reward for Episode 7: 1.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 7: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 7: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 7: 3.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 7: 3.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 7: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 7: 4.0\n",
      "Step Reward: 1 | Total Reward for Episode 7: 5.0\n",
      "Step Reward: 1 | Total Reward for Episode 7: 6.0\n",
      "Step Reward: -10 | Total Reward for Episode 7: -4.0\n",
      "Step Reward: 1 | Total Reward for Episode 8: 1\n",
      "Step Reward: 0.0 | Total Reward for Episode 8: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 8: 2.0\n",
      "Step Reward: -1 | Total Reward for Episode 8: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 8: 2.0\n",
      "Step Reward: -1 | Total Reward for Episode 8: 1.0\n",
      "Step Reward: -1 | Total Reward for Episode 8: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 8: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 8: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 8: 0.0\n",
      "Step Reward: 1 | Total Reward for Episode 8: 1.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 8: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 8: 2.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 8: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 8: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 8: 4.0\n",
      "Step Reward: 1 | Total Reward for Episode 8: 5.0\n",
      "Step Reward: 1 | Total Reward for Episode 8: 6.0\n",
      "Step Reward: -10 | Total Reward for Episode 8: -4.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 1\n",
      "Step Reward: 0.0 | Total Reward for Episode 9: 1.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 9: 1.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 9: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 2.0\n",
      "Step Reward: -1 | Total Reward for Episode 9: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 3.0\n",
      "Step Reward: -1 | Total Reward for Episode 9: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 4.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 5.0\n",
      "Step Reward: -1 | Total Reward for Episode 9: 4.0\n",
      "Step Reward: -1 | Total Reward for Episode 9: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 4.0\n",
      "Step Reward: -1 | Total Reward for Episode 9: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Reward: -1 | Total Reward for Episode 9: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 4.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 5.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 6.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 7.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 8.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 9.0\n",
      "Step Reward: -1 | Total Reward for Episode 9: 8.0\n",
      "Step Reward: 1 | Total Reward for Episode 9: 9.0\n",
      "Step Reward: -1 | Total Reward for Episode 9: 8.0\n",
      "Step Reward: -10 | Total Reward for Episode 9: -2.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 10: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 10: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 10: 0.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 10: 0.0\n",
      "Step Reward: 1 | Total Reward for Episode 10: 1.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 10: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 10: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 10: 3.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 10: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 10: 4.0\n",
      "Step Reward: -10 | Total Reward for Episode 10: -6.0\n",
      "Step Reward: 1 | Total Reward for Episode 11: 1\n",
      "Step Reward: -1 | Total Reward for Episode 11: 0\n",
      "Step Reward: 1 | Total Reward for Episode 11: 1\n",
      "Step Reward: 0.0 | Total Reward for Episode 11: 1.0\n",
      "Step Reward: 1 | Total Reward for Episode 11: 2.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 11: 2.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 11: 2.0\n",
      "Step Reward: 1 | Total Reward for Episode 11: 3.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 11: 3.0\n",
      "Step Reward: 0.0 | Total Reward for Episode 11: 3.0\n",
      "Step Reward: 1 | Total Reward for Episode 11: 4.0\n",
      "Step Reward: -10 | Total Reward for Episode 11: -6.0\n",
      "Step Reward: 1 | Total Reward for Episode 12: 1\n",
      "Step Reward: 1 | Total Reward for Episode 12: 2\n",
      "Step Reward: 1 | Total Reward for Episode 12: 3\n",
      "Step Reward: -1 | Total Reward for Episode 12: 2\n"
     ]
    }
   ],
   "source": [
    "# Advanced Version\n",
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "def run(episodes, is_training=True, render=False):\n",
    "\n",
    "    env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True, render_mode='human' if render else None)\n",
    "\n",
    "    if is_training:\n",
    "        q = np.zeros((env.observation_space.n, env.action_space.n))  # init a 64 x 4 array\n",
    "    else:\n",
    "        with open('frozen_lake8x8.pkl', 'rb') as f:\n",
    "            q = pickle.load(f)\n",
    "\n",
    "    learning_rate_a = 0.9  # alpha or learning rate\n",
    "    discount_factor_g = 0.9  # gamma or discount rate\n",
    "    epsilon = 1.0  # Start with 100% random actions\n",
    "    epsilon_decay_rate = 0.00001  # Epsilon decay rate\n",
    "    rng = np.random.default_rng()  # Random number generator\n",
    "\n",
    "    goal_position = 63  # The goal is the bottom-right corner in the 8x8 grid (index 63)\n",
    "    \n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "    total_rewards = []  # Store total rewards for each episode\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state = env.reset()[0]  # Get initial state\n",
    "        terminated = False  # True when fall in hole or reached goal\n",
    "        truncated = False  # True when actions > 200\n",
    "        episode_reward = 0  # Reward for the current episode\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            if is_training and rng.random() < epsilon:\n",
    "                action = env.action_space.sample()  # Explore: Random action\n",
    "            else:\n",
    "                action = np.argmax(q[state, :])  # Exploit: Best action\n",
    "\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # Calculate distance to goal\n",
    "            current_distance = abs(goal_position - state)\n",
    "            new_distance = abs(goal_position - new_state)\n",
    "\n",
    "            # Rewards and penalties based on the agent's movement\n",
    "            if terminated and new_state == goal_position:\n",
    "                reward = 100  # Reached the goal, +100 points\n",
    "            elif terminated and reward == 0:  # Fell into a hole\n",
    "                reward = -10  # Fell into a hole, -10 penalty\n",
    "            elif new_distance < current_distance:\n",
    "                reward = 1  # Moving closer to the goal, +1 point\n",
    "            elif new_distance > current_distance:\n",
    "                reward = -1  # Moving farther from the goal, -1 penalty\n",
    "\n",
    "            if is_training:\n",
    "                q[state, action] = q[state, action] + learning_rate_a * (\n",
    "                    reward + discount_factor_g * np.max(q[new_state, :]) - q[state, action]\n",
    "                )\n",
    "\n",
    "            state = new_state\n",
    "            episode_reward += reward  # Accumulate reward for this episode\n",
    "\n",
    "            # Render the environment and display the current reward if render=True\n",
    "            if render:\n",
    "                env.render()\n",
    "                print(f\"Step Reward: {reward} | Total Reward for Episode {i+1}: {episode_reward}\")\n",
    "                time.sleep(0.1)  # Adding a small delay to see the rendering\n",
    "\n",
    "        # Update epsilon and learning rate\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "\n",
    "        if epsilon == 0:\n",
    "            learning_rate_a = 0.0001\n",
    "\n",
    "        rewards_per_episode[i] = episode_reward  # Store reward for this episode\n",
    "        total_rewards.append(episode_reward)  # Add to the total rewards list\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Print the total rewards at the end of the run\n",
    "    print(f\"Total rewards over {episodes} episodes: {np.sum(total_rewards)}\")\n",
    "    print(f\"Average reward per episode: {np.mean(total_rewards)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Training with rendering to visualize rewards in real-time\n",
    "    run(10000, is_training=True, render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c197717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
