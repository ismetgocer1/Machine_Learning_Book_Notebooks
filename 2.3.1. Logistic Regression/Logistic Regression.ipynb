{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365d165f",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68498a35",
   "metadata": {},
   "source": [
    "This NB uses;\n",
    "0) Pipline for handling null values both categorical and numeric columns + encoding + analysis. So, if you want to deploy i on Streamlit, you will not need to call encoder. Just calling model is enough. \n",
    "\n",
    "1) \"class_weight=\"balanced\" \" and\n",
    "\n",
    "2) \"stratfy = y\" for handling unbalance at the target variable. Thus, it can be use at \"balanced\" an \"unbalanced\" data.\n",
    "\n",
    "3) Target variable is \"Default\", meaning taht customer will not pay credit. 0--->No Default Risk, 1---> Default\n",
    "\n",
    "4) We don't need apply scaling in tree base models.\n",
    "\n",
    "5) We don't need apply encoding on Target variable in tree base models.\n",
    "\n",
    "6) We should use ordinal encoding on categorical variables in features. But this data set not including any categorical data as feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce2dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL METRICS FOR BASE MODEL\n",
      "Evaluation metrics for Train data:\n",
      "Accuracy: 0.605\n",
      "F1 Score: 0.24761904761904763\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.60      0.73       359\n",
      "           1       0.15      0.63      0.25        41\n",
      "\n",
      "    accuracy                           0.60       400\n",
      "   macro avg       0.54      0.62      0.49       400\n",
      "weighted avg       0.85      0.60      0.68       400\n",
      "\n",
      "Confusion Matrix:\n",
      "[[216 143]\n",
      " [ 15  26]]\n",
      "\n",
      "\n",
      "Evaluation metrics for Test data:\n",
      "Accuracy: 0.57\n",
      "F1 Score: 0.21818181818181817\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.57      0.70        90\n",
      "           1       0.13      0.60      0.22        10\n",
      "\n",
      "    accuracy                           0.57       100\n",
      "   macro avg       0.53      0.58      0.46       100\n",
      "weighted avg       0.85      0.57      0.65       100\n",
      "\n",
      "Confusion Matrix:\n",
      "[[51 39]\n",
      " [ 4  6]]\n",
      "\n",
      "\n",
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__solver': 'lbfgs'}\n",
      "\n",
      "----EVAL METRICS FOR BEST MODEL----\n",
      "\n",
      "Evaluation metrics for Train (Best Model) data:\n",
      "Accuracy: 0.605\n",
      "F1 Score: 0.24761904761904763\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.94      0.73       231\n",
      "           1       0.63      0.15      0.25       169\n",
      "\n",
      "    accuracy                           0.60       400\n",
      "   macro avg       0.62      0.54      0.49       400\n",
      "weighted avg       0.62      0.60      0.53       400\n",
      "\n",
      "Confusion Matrix:\n",
      "[[216  15]\n",
      " [143  26]]\n",
      "\n",
      "\n",
      "Evaluation metrics for Test (Best Model) data:\n",
      "Accuracy: 0.57\n",
      "F1 Score: 0.21818181818181817\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.93      0.70        55\n",
      "           1       0.60      0.13      0.22        45\n",
      "\n",
      "    accuracy                           0.57       100\n",
      "   macro avg       0.58      0.53      0.46       100\n",
      "weighted avg       0.58      0.57      0.49       100\n",
      "\n",
      "Confusion Matrix:\n",
      "[[51  4]\n",
      " [39  6]]\n",
      "\n",
      "\n",
      "     Metric  BaseModelTrain  BaseModelTest  BestModelTrain  BestModelTest\n",
      "0  Accuracy        0.605000       0.570000        0.605000       0.570000\n",
      "1  F1 Score        0.247619       0.218182        0.247619       0.218182\n",
      " \n",
      "Final model saved as 'final_logistic_regression_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import joblib\n",
    "\n",
    "import warnings  # for ignoring unnecessary warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "\n",
    "# ------------------------ #\n",
    "\n",
    "# 1) Load the dataset \n",
    "df = pd.read_excel('sample_data.xlsx') \n",
    "# Ensure the target variable is named \"Target\" in the Excel file.\n",
    "\n",
    "# 2) Separate features and target variable from the dataset\n",
    "X = df.drop(columns=[\"Target\"])\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# 3) Create transformers for data preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# 4) Create a pipeline with a Logistic Regression model\n",
    "base_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(class_weight=\"balanced\", random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# 5) Perform a train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 6) Train and evaluate the Base Model\n",
    "base_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7) Predictions and evaluation metrics\n",
    "print(\"EVAL METRICS FOR BASE MODEL\")\n",
    "y_train_pred = base_pipeline.predict(X_train)\n",
    "y_test_pred = base_pipeline.predict(X_test)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, data_type=\"Train\"):\n",
    "    print(f\"Evaluation metrics for {data_type} data:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"F1 Score: {f1_score(y_true, y_pred)}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\n\")\n",
    "\n",
    "evaluate_model(y_train, y_train_pred, \"Train\")\n",
    "evaluate_model(y_test, y_test_pred, \"Test\")\n",
    "\n",
    "# 8) Hyperparameter Tuning (for Logistic Regression)\n",
    "param_grid = {\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "    'classifier__solver': ['liblinear', 'lbfgs']     # Solvers for logistic regression\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(base_pipeline, param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# 9) Build and evaluate the Best Model\n",
    "print(\"\")\n",
    "print(\"----EVAL METRICS FOR BEST MODEL----\")\n",
    "print(\"\")\n",
    "\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "y_train_best_pred = best_pipeline.predict(X_train)\n",
    "y_test_best_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "evaluate_model(y_train_best_pred, y_train, \"Train (Best Model)\")\n",
    "evaluate_model(y_test_best_pred, y_test, \"Test (Best Model)\")\n",
    "\n",
    "# Compare metrics\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score'],\n",
    "    'BaseModelTrain': [accuracy_score(y_train, y_train_pred), f1_score(y_train, y_train_pred)],\n",
    "    'BaseModelTest': [accuracy_score(y_test, y_test_pred), f1_score(y_test, y_test_pred)],\n",
    "    'BestModelTrain': [accuracy_score(y_train, y_train_best_pred), f1_score(y_train, y_train_best_pred)],\n",
    "    'BestModelTest': [accuracy_score(y_test, y_test_best_pred), f1_score(y_test, y_test_best_pred)]\n",
    "})\n",
    "\n",
    "print(metrics_comparison)\n",
    "\n",
    "# 10) Train the Final Model on the entire dataset\n",
    "final_model = best_pipeline.fit(X, y)\n",
    "\n",
    "# 11) Save the Final Model\n",
    "joblib.dump(final_model, 'final_logistic_regression_model.pkl')\n",
    "\n",
    "print(\" \")\n",
    "print(\"Final model saved as 'final_logistic_regression_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394bce05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
